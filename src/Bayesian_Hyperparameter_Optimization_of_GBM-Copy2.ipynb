{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "49BK-d6741hR"
   },
   "source": [
    "# Introduction: Automated GBM Hyperparameter Optimization\n",
    "\n",
    "中文教程：https://www.jiqizhixin.com/articles/2018-08-08-2 \n",
    "\n",
    "lgb递进调参：http://www.imooc.com/article/43784 xgboost调参：https://blog.csdn.net/u010665216/article/details/78532619?utm_source=blogxgwz4\n",
    "\n",
    "lgb参考2：https://www.cnblogs.com/wanglei5205/p/8722237.html\n",
    "\n",
    "In this notebook we will walk through automated hyperparameter tuning using Bayesian Optimization. Specifically, we will optimize the hyperparameters of a Gradient Boosting Machine using the Hyperopt library (with the Tree Parzen Estimator algorithm).  We will compare the results of random search (implemented manually) for hyperparameter tuning with the Bayesian model-based optimization method to try and understand how the Bayesian method works and what benefits it has over uninformed search methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tLgpzCWY41hX"
   },
   "source": [
    "## Hyperopt\n",
    "\n",
    "Hyperopt is one of several automated hyperparameter tuning libraries using Bayesian optimization. These libraries differ in the algorithm used to both construct the surrogate (probability model) of the objective function and choose the next hyperparameters to evaluate in the objective function. Hyperopt uses the Tree Parzen Estimator (TPE). Other Python libraries include Spearmint, which uses a Gaussian process for the surrogate, and SMAC, which uses a random forest regression. \n",
    "\n",
    "Hyperopt has a simple syntax for structuring an optimization problem which extends beyond hyperparameter tuning to any problem that involves minimizing a function. Moreover, the structure of a Bayesian Optimization problem is similar across the libraries, with the major differences coming in the syntax (and in the algorithms behind the scenes that we do not have to deal with). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "vBDwNlro41hZ",
    "outputId": "650f131b-568a-4a45-f104-8a3ed15255b8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Modeling\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Evaluation of the model\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "MAX_EVALS = 200\n",
    "N_FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wanshun/my_code/mobile/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jR-Mi-fh41hg"
   },
   "source": [
    "## Data\n",
    "\n",
    "For this notebook we will work with only the `application` data. The methods developed here can work on any dataset but the run-times can be long! We will also separate the training set into training and testing to evaluate the performance of hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PXEbe4hh41hi",
    "outputId": "7fde9130-d316-4d40-f534-abcc9dfb7c67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pop column is  ['former_complaint_fee', 'former_complaint_num', 'complaint_level']\n",
      "Train shape:  (396635, 22)\n",
      "Test shape:  (67163, 22)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_traffic</th>\n",
       "      <th>1_total_fee</th>\n",
       "      <th>2_total_fee</th>\n",
       "      <th>3_total_fee</th>\n",
       "      <th>4_total_fee</th>\n",
       "      <th>age</th>\n",
       "      <th>contract_time</th>\n",
       "      <th>local_caller_time</th>\n",
       "      <th>local_trafffic_month</th>\n",
       "      <th>online_time</th>\n",
       "      <th>...</th>\n",
       "      <th>service1_caller_time</th>\n",
       "      <th>service2_caller_time</th>\n",
       "      <th>last_month_traffic</th>\n",
       "      <th>service_type</th>\n",
       "      <th>is_mix_service</th>\n",
       "      <th>many_over_bill</th>\n",
       "      <th>contract_type</th>\n",
       "      <th>is_promise_low_consume</th>\n",
       "      <th>net_service</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1276.235687</td>\n",
       "      <td>137.35</td>\n",
       "      <td>136.00</td>\n",
       "      <td>156.00</td>\n",
       "      <td>195.95</td>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "      <td>472.383333</td>\n",
       "      <td>1276.275303</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>5.583333</td>\n",
       "      <td>638.933333</td>\n",
       "      <td>746.466083</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>443.197619</td>\n",
       "      <td>96.00</td>\n",
       "      <td>96.00</td>\n",
       "      <td>125.10</td>\n",
       "      <td>96.09</td>\n",
       "      <td>48</td>\n",
       "      <td>12</td>\n",
       "      <td>149.783333</td>\n",
       "      <td>435.072411</td>\n",
       "      <td>134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>171.516667</td>\n",
       "      <td>1024.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1296.080153</td>\n",
       "      <td>106.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>18.350000</td>\n",
       "      <td>1296.081185</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.766667</td>\n",
       "      <td>735.982041</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>416.605755</td>\n",
       "      <td>136.10</td>\n",
       "      <td>157.45</td>\n",
       "      <td>120.00</td>\n",
       "      <td>120.00</td>\n",
       "      <td>44</td>\n",
       "      <td>24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>891.790241</td>\n",
       "      <td>446.92</td>\n",
       "      <td>544.54</td>\n",
       "      <td>329.33</td>\n",
       "      <td>512.90</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>892.061959</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>54.066667</td>\n",
       "      <td>12.666667</td>\n",
       "      <td>7680.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   month_traffic  1_total_fee  2_total_fee  3_total_fee  4_total_fee  age  \\\n",
       "0    1276.235687       137.35       136.00       156.00       195.95   37   \n",
       "1     443.197619        96.00        96.00       125.10        96.09   48   \n",
       "2    1296.080153       106.00       106.00       106.00       106.00   20   \n",
       "3     416.605755       136.10       157.45       120.00       120.00   44   \n",
       "4     891.790241       446.92       544.54       329.33       512.90   28   \n",
       "\n",
       "   contract_time  local_caller_time  local_trafffic_month  online_time  \\\n",
       "0             12         472.383333           1276.275303           10   \n",
       "1             12         149.783333            435.072411          134   \n",
       "2              0          18.350000           1296.081185           17   \n",
       "3             24           0.000000              0.000000           51   \n",
       "4              0           2.700000            892.061959           66   \n",
       "\n",
       "    ...    service1_caller_time  service2_caller_time  last_month_traffic  \\\n",
       "0   ...                5.583333            638.933333          746.466083   \n",
       "1   ...                0.000000            171.516667         1024.000000   \n",
       "2   ...                0.000000             20.766667          735.982041   \n",
       "3   ...                0.000000             60.800000            0.000000   \n",
       "4   ...               54.066667             12.666667         7680.000000   \n",
       "\n",
       "   service_type  is_mix_service  many_over_bill  contract_type  \\\n",
       "0             4               1               0              6   \n",
       "1             4               0               0              3   \n",
       "2             4               0               0              0   \n",
       "3             4               0               0              1   \n",
       "4             4               0               1              0   \n",
       "\n",
       "   is_promise_low_consume  net_service  gender  \n",
       "0                       0            4       1  \n",
       "1                       0            4       1  \n",
       "2                       0            4       2  \n",
       "3                       1            4       1  \n",
       "4                       0            4       0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data and separate into training and testing sets\n",
    "path = '../data/'\n",
    "train = pd.read_csv(path + 'datas_8classes.csv')\n",
    "test = pd.read_csv(path + 'test_datas_8classes.csv')\n",
    "\n",
    "classes = sorted(list(set(train['current_service'])))\n",
    "label2current_service = dict(zip(range(0,len(classes)), classes))\n",
    "current_service2label = dict(zip(classes,range(0,len(classes))))\n",
    "\n",
    "train['current_service'] = train['current_service'].map(current_service2label)\n",
    "\n",
    "\n",
    "y = train.pop('current_service')\n",
    "train_id = train.pop('user_id')\n",
    "\n",
    "X = train\n",
    "train_col_tmp = ['former_complaint_fee','former_complaint_num','complaint_level']\n",
    "\n",
    "print('pop column is ',train_col_tmp)\n",
    "for itm in  train_col_tmp:\n",
    "    train.pop(itm)\n",
    "train_col = train.columns\n",
    "features_num = len(train_col)\n",
    "X_test = test[train_col]\n",
    "test_id = test['user_id']\n",
    "\n",
    "\n",
    "# for i in train_col:\n",
    "    # X[i] = X[i].replace(\"\\\\N\",-1)\n",
    "    # X_test[i] = X_test[i].replace(\"\\\\N\",-1)\n",
    "\n",
    "train_features,y,test_features = X,y,X_test\n",
    "\n",
    "print('Train shape: ', train_features.shape)\n",
    "print('Test shape: ', test_features.shape)\n",
    "\n",
    "train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a lgb dataset\n",
    "le_train_set = lgb.Dataset(train_features, label=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ua_bd_gN41hx"
   },
   "source": [
    "All we need to do is fit the model on the training data and make predictions on the testing data. For the predictions, because we are measuring ROC AUC and not accuracy, we have the model predict probabilities and not hard binary values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yzsh90dG41kJ"
   },
   "source": [
    "Now, we can move on to Bayesian methods and see if they are able to achieve better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DOWEfWsF41kK"
   },
   "source": [
    "# Bayesian Hyperparameter Optimization using Hyperopt\n",
    "\n",
    "For Bayesian optimization in Hyperopt, we need the following four parts:\n",
    "\n",
    "1. Objective function\n",
    "2. Domain space\n",
    "3. Hyperparameter optimization algorithm\n",
    "4. History of results\n",
    "\n",
    "We already used all of these in random search, but for Hyperopt we will have to make a few changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0sRGTWN41kM"
   },
   "source": [
    "## Objective Function \n",
    "\n",
    "This objective function will still take in the hyperparameters but it will return not a list but a dictionary. The only requirement for an objective function in Hyperopt is that it has a key in the return dictionary called `\"loss\"` to minimize and a key called `\"status\"` indicating if the evaluation was successful. \n",
    "\n",
    "If we want to keep track of the number of iterations, we can declare a global variables called `ITERATION` that is incremented every time the function is called. In addition to returning comprehensive results, every time the function is evaluated, we will write the results to a new line of a csv file. This can be useful for extremely long evaluations if we want to check on the progress (this might not be the most elegant solution, but it's better than printing to the console because our results will be saved!) \n",
    "\n",
    "The most important part of this function is that now we need to return a __value to minimize__ and not the raw ROC AUC. We are trying to find the best value of the objective function, and even though a higher ROC AUC is better, Hyperopt works to minimize a function. Therefore, a simple solution is to return 1 - ROC (we did this for random search as well for practice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "bjH8xcfC41kO"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from hyperopt import STATUS_OK\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_vali(preds, data_vali):\n",
    "    labels = data_vali.get_label()\n",
    "    preds = np.argmax(preds.reshape(len(classes), -1), axis=0)\n",
    "    score_vali = f1_score(y_true=labels, y_pred=preds, average=None)\n",
    "    print(score_vali)\n",
    "    score_vali = np.mean(score_vali)\n",
    "    score_vali = score_vali * score_vali\n",
    "    return 'f1_score', score_vali, True\n",
    "\n",
    "def objective(params, n_folds = N_FOLDS):\n",
    "    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Optimization\"\"\"\n",
    "    \n",
    "    # Keep track of evals\n",
    "    global ITERATION\n",
    "    \n",
    "    ITERATION += 1\n",
    "    \n",
    "    encoding_type = params['encoding']\n",
    "    \n",
    "    # Handle the encoding\n",
    "    if encoding_type == 'one_hot':\n",
    "        train_set = oh_train_set\n",
    "    elif encoding_type == 'label':\n",
    "        train_set = le_train_set\n",
    "    \n",
    "    del params['encoding']\n",
    "    \n",
    "    # Retrieve the subsample\n",
    "    subsample = params['boosting_type'].get('subsample', 1.0)\n",
    "    \n",
    "    # Extract the boosting type and subsample to top level keys\n",
    "    params['boosting_type'] = params['boosting_type']['boosting_type']\n",
    "    params['subsample'] = subsample\n",
    "    \n",
    "    params['objective'] = \"multiclass\"\n",
    "    params['metric'] = \"multi_logloss\"\n",
    "    params['num_class'] = len(classes)\n",
    "    params['is_unbalance'] = True\n",
    "    params['verbosity'] = -1\n",
    "    params[\"n_jobs\"] = 4\n",
    "    params['device'] = 'gpu'\n",
    "    # \"gpu_platform_id\":-1,\n",
    "    # \"gpu_device_id\":-1\n",
    "    # 'gpu_use_dp':False\n",
    "    \n",
    "    # Make sure parameters that need to be integers are integers\n",
    "    for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n",
    "        params[parameter_name] = int(params[parameter_name])\n",
    "    \n",
    "    start = timer()\n",
    "    \n",
    "    # Perform n_folds cross validation\n",
    "    cv_results = lgb.cv(params, train_set, num_boost_round = 1000, nfold = n_folds, \n",
    "                        early_stopping_rounds = 100, feval=f1_score_vali, seed = 24)\n",
    "    \n",
    "    run_time = timer() - start\n",
    "    \n",
    "    # Extract the best score\n",
    "    best_score = np.max(cv_results['f1_score-mean'])\n",
    "    \n",
    "    # Loss must be minimized\n",
    "    loss = 1 - best_score\n",
    "    \n",
    "    # Boosting rounds that returned the highest cv score\n",
    "    n_estimators = int(np.argmax(cv_results['f1_score-mean']) + 1)\n",
    "    \n",
    "    params['encoding'] = encoding_type\n",
    "    \n",
    "    if ITERATION % 100 == 0:\n",
    "        # Display the information\n",
    "        display('Iteration {}: {} Fold CV f1_score {:.5f}'.format(ITERATION, N_FOLDS, best_score))\n",
    "\n",
    "    # Write to the csv file ('a' means append)\n",
    "    of_connection = open(out_file, 'a')\n",
    "    writer = csv.writer(of_connection)\n",
    "    writer.writerow([loss, params, ITERATION, n_estimators, run_time, best_score])\n",
    "    of_connection.close()\n",
    "    \n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'params': params, 'iteration': ITERATION,\n",
    "            'estimators': n_estimators, \n",
    "            'train_time': run_time, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vBlkoDBf41kR"
   },
   "source": [
    "Although Hyperopt only needs the `loss`, it's a good idea to track other metrics so we can inspect the results. Later we can compare the sequence of searches to that from random search which will help us understand how the method works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ODmo-6_o41kS"
   },
   "source": [
    "## Domain Space\n",
    "\n",
    "Specifying the domain (called the `space` in Hyperopt) is a little trickier than in grid search. In Hyperopt, and other Bayesian optimization frameworks, the domian is not a discrete grid but instead has probability distributions for each hyperparameter. For each hyperparameter, we will use the same limits as with the grid, but instead of being defined at each point, the domain represents probabilities for each hyperparameter. This will probably become clearer in the code and the images! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "WvX8AYgp41kU"
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "from hyperopt.pyll.stochastic import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HHkYVKbY41kf"
   },
   "source": [
    "First we will go through an example of the learning rate. Again, we are using a log-uniform space for the learning rate defined from 0.005 to 0.2 (same as with the grid.) This time, when we graph the domain, it's more accurate to see a kernel density estimate plot than a histogram (although both show distributions). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "JGvJte_u41kg"
   },
   "outputs": [],
   "source": [
    "# Create the learning rate\n",
    "learning_rate = {'learning_rate': hp.loguniform('learning_rate', np.log(0.005), np.log(0.2))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AVTz8Fdy41kj"
   },
   "source": [
    "We can visualize the learning rate by sampling from the space using a Hyperopt utility. Here we plot 10000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W7O6VRPj41kk"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAAGMCAYAAACs4hrEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f39ee688978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate_dist = []\n",
    "\n",
    "# Draw 10000 samples from the learning rate domain\n",
    "for _ in range(10000):\n",
    "    learning_rate_dist.append(sample(learning_rate)['learning_rate'])\n",
    "    \n",
    "plt.figure(figsize = (8, 6))\n",
    "sns.kdeplot(learning_rate_dist, color = 'red', linewidth = 2, shade = True);\n",
    "plt.title('Learning Rate Distribution', size = 18); plt.xlabel('Learning Rate', size = 16); plt.ylabel('Density', size = 16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UJPd3daA41km"
   },
   "source": [
    "The number of leaves is again a uniform distribution. Here we used `quniform` which means a discrete uniform (as opposed to continuous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VYpNNKaw41kn"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgcAAAGMCAYAAABOPgG9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f39ee766da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Discrete uniform distribution\n",
    "num_leaves = {'num_leaves': hp.quniform('num_leaves', 30, 150, 1)}\n",
    "num_leaves_dist = []\n",
    "\n",
    "# Sample 10000 times from the number of leaves distribution\n",
    "for _ in range(10000):\n",
    "    num_leaves_dist.append(sample(num_leaves)['num_leaves'])\n",
    "    \n",
    "# kdeplot\n",
    "plt.figure(figsize = (8, 6))\n",
    "sns.kdeplot(num_leaves_dist, linewidth = 2, shade = True);\n",
    "plt.title('Number of Leaves Distribution', size = 18); plt.xlabel('Number of Leaves', size = 16); plt.ylabel('Density', size = 16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uk4yYULV41kr"
   },
   "source": [
    "### Conditional Domain\n",
    "\n",
    "In Hyperopt, we can use nested conditional statements to indicate hyperparameters that depend on other hyperparameters. For example, we know that `goss` boosting type cannot use subsample, so when we set up the `boosting_type` categorical variable, we have to se the `subsample` to 1.0 while for the other boosting types it's a float between 0.5 and 1.0 Let's see this with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q_9CZUwT41ks"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': {'boosting_type': 'gbdt', 'subsample': 0.975378771456646}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# boosting type domain \n",
    "boosting_type = {'boosting_type': hp.choice('boosting_type', \n",
    "                                            [{'boosting_type': 'gbdt', 'subsample': hp.uniform('subsample', 0.5, 1)}, \n",
    "                                             {'boosting_type': 'dart', 'subsample': hp.uniform('subsample', 0.5, 1)},\n",
    "                                             {'boosting_type': 'goss', 'subsample': 1.0}])}\n",
    "\n",
    "# Draw a sample\n",
    "params = sample(boosting_type)\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S7ABvql341kv"
   },
   "source": [
    "We need to set both the `boosting_type` and `subsample` as top-level keys in the parameter dictionary. We can use the Python `dict.get` method with a default value of 1.0. This means that if the key is not present in the dictionary, the value returned will be the default (1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lynXjFDF41kw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt', 'subsample': 0.975378771456646}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the subsample if present otherwise set to 1.0\n",
    "subsample = params['boosting_type'].get('subsample', 1.0)\n",
    "\n",
    "# Extract the boosting type\n",
    "params['boosting_type'] = params['boosting_type']['boosting_type']\n",
    "params['subsample'] = subsample\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TJx_WQMf41k1"
   },
   "source": [
    "This is because the gbm cannot use the nested dictionary so we need to set the `boosting_type` and `subsample` as top level keys. Nested conditionals allow us to use a different set of hyperparameters depending on other hyperparameters. For example, we can explore different models with completely different sets of hyperparameters by using nested conditionals. The only requirement is that the first nested statement must be based on a `choice` hyperparameter (the choice could be the type of model).\n",
    "\n",
    "## Complete Bayesian Domain\n",
    "\n",
    "Now we can define the entire domain. Each variable needs to have a label and a few parameters specifying the type and extent of the distribution. For the variables such as boosting type that are categorical, we use the `choice` variable. Other variables types include `quniform`, `loguniform`, and `uniform`. For the complete list, check out the [documentation](https://github.com/hyperopt/hyperopt/wiki/FMin) for Hyperopt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "hg-FbzPe41k2"
   },
   "outputs": [],
   "source": [
    "# Define the search space\n",
    "space = {\n",
    "    'encoding': hp.choice('encoding', ['label']),\n",
    "    'class_weight': hp.choice('class_weight', [None, 'balanced']),\n",
    "    'boosting_type': hp.choice('boosting_type', \n",
    "                                            [{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}]),\n",
    "    'num_leaves': hp.quniform('num_leaves', 30, 150, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.1), np.log(0.2)),\n",
    "    'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n",
    "    'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ew6ZMfFw41k5"
   },
   "source": [
    "### Example of Sampling from the Domain \n",
    "\n",
    "Let's sample from the domain (using the conditional logic) to see the result of each draw. Every time we run this code, the results will change. (Again notice that we need to assign the top level keys to the keywords understood by the GBM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ntzwLGXo41k5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt',\n",
       " 'class_weight': 'balanced',\n",
       " 'colsample_bytree': 0.799586515091031,\n",
       " 'encoding': 'label',\n",
       " 'learning_rate': 0.10512523191893401,\n",
       " 'min_child_samples': 340.0,\n",
       " 'num_leaves': 60.0,\n",
       " 'reg_alpha': 0.11224318940698397,\n",
       " 'reg_lambda': 0.8925990796997245,\n",
       " 'subsample': 0.6282036995243927,\n",
       " 'subsample_for_bin': 20000.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample from the full space\n",
    "x = sample(space)\n",
    "\n",
    "# Conditional logic to assign top-level keys\n",
    "subsample = x['boosting_type'].get('subsample', 1.0)\n",
    "x['boosting_type'] = x['boosting_type']['boosting_type']\n",
    "x['subsample'] = subsample\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GOZldgm741k-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt',\n",
       " 'class_weight': None,\n",
       " 'colsample_bytree': 0.6192440853524482,\n",
       " 'encoding': 'label',\n",
       " 'learning_rate': 0.11899529775420577,\n",
       " 'min_child_samples': 410.0,\n",
       " 'num_leaves': 40.0,\n",
       " 'reg_alpha': 0.6066987885761421,\n",
       " 'reg_lambda': 0.22202461282423602,\n",
       " 'subsample': 0.5998382024267694,\n",
       " 'subsample_for_bin': 300000.0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sample(space)\n",
    "subsample = x['boosting_type'].get('subsample', 1.0)\n",
    "x['boosting_type'] = x['boosting_type']['boosting_type']\n",
    "x['subsample'] = subsample\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "amx7A3Ln41lC"
   },
   "source": [
    "## Optimization Algorithm\n",
    "\n",
    "Although this is the most technical part of Bayesian optimization, defining the algorithm to use in Hyperopt is simple. We will use the Tree Parzen Estimator (read about it [in this paper](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf)) which is one method for constructing the surrogate function and choosing the next hyperparameters to evaluate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "9ACHkdQk41lD"
   },
   "outputs": [],
   "source": [
    "from hyperopt import tpe\n",
    "\n",
    "tpe_algorithm = tpe.suggest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EWofAWlX41lG"
   },
   "source": [
    "## Results History\n",
    "\n",
    "The final part is the result history. Here, we are using two methods to make sure we capture all the results:\n",
    "\n",
    "1. A `Trials` object that stores the dictionary returned from the objective function\n",
    "2. Writing to a csv file every iteration\n",
    "\n",
    "The csv file option also lets us monitor the results of an on-going experiment. (Although do not use Excel to open the file while training is on-going. Instead check the results using `tail results/gbm_trials.csv` from bash or another command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "0FiS4ji841lG"
   },
   "outputs": [],
   "source": [
    "from hyperopt import Trials\n",
    "\n",
    "trials = Trials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vpFt6wxB41lJ"
   },
   "source": [
    "The `Trials` object will hold everything returned from the objective function in the `.results` attribute. It also holds other information from the search, but we return everything we need from the objective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "vc1y9AwU41lL"
   },
   "outputs": [],
   "source": [
    "# File to save first results\n",
    "out_file = '../result/gbm_f1_results_kaggle.csv'\n",
    "of_connection = open(out_file, 'w')\n",
    "writer = csv.writer(of_connection)\n",
    "\n",
    "# Write the headers to the file\n",
    "writer.writerow(['loss', 'params', 'iteration', 'estimators', 'time', 'ROC AUC'])\n",
    "of_connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xMVKWlUe41lN"
   },
   "source": [
    "Every time the objective function is called, it will write one line to this file. Running the cell above does clear the file though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QC-rmNbJ41lO"
   },
   "source": [
    "## Bayesian Optimization\n",
    "\n",
    "We have everything in place needed to run the optimization. First we declare the global variable that will be used to keep track of the number of iterations. Then, we call `fmin` passing in everything we defined above and the maximum number of iterations to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "d8F0xWpZ41lQ"
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_usrrqt441lT",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Global variable\n",
    "global  ITERATION\n",
    "\n",
    "ITERATION = 0\n",
    "\n",
    "# Run optimization\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, \n",
    "            max_evals = MAX_EVALS, trials = trials, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H5EscBZk41lV"
   },
   "source": [
    "The `.results` attribute of the `Trials` object has all information from the objective function. If we sort this by the lowest loss, we can see the hyperparameters that performed the best in terms of validation loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hnh7tBHq41lW"
   },
   "outputs": [],
   "source": [
    "# Sort the trials with lowest loss (highest AUC) first\n",
    "trials_results = sorted(trials.results, key = lambda x: x['loss'])\n",
    "trials_results[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3H-O1loz41lY"
   },
   "source": [
    "We can also access the results from the csv file (which might be easier since it's already a dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "qA4eNMoY41lZ"
   },
   "outputs": [],
   "source": [
    "results = pd.read_csv('../result/gbm_f1_results_kaggle.csv')\n",
    "\n",
    "# Sort with best scores on top and reset index for slicing\n",
    "results.sort_values('loss', ascending = True, inplace = True)\n",
    "results.reset_index(inplace = True, drop = True)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H3EDFGNF41lc"
   },
   "source": [
    "For some reason, when we save to a file and then read back in, the dictionary of hyperparameters is represented as a string. To convert from a string back to a dictionary we can use the `ast` library and the `literal_eval` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "08T4RQxO41lc"
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Convert from a string to a dictionary\n",
    "ast.literal_eval(results.loc[0, 'params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FULCSPXR41le"
   },
   "source": [
    "## Evaluate Best Results\n",
    "\n",
    "Now for the moment of truth: did the optimization pay off? For this problem with a relatively small dataset, the benefits of hyperparameter optimization compared to random search are probably minor (if there are any). Random search might turn up a better result in fewer iterations simply becuase of randomness! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "pO2WpUmC41le"
   },
   "outputs": [],
   "source": [
    "# Extract the ideal number of estimators and hyperparameters\n",
    "best_bayes_estimators = int(results.loc[0, 'estimators'])\n",
    "best_bayes_params = ast.literal_eval(results.loc[0, 'params']).copy()\n",
    "\n",
    "# Re-create the best model and train on the training data\n",
    "best_bayes_model = lgb.LGBMClassifier(n_estimators=best_bayes_estimators, \n",
    "                                      n_jobs = -1, \n",
    "                                      objective = 'binary', \n",
    "                                      random_state = 50, \n",
    "                                      **best_bayes_params)\n",
    "best_bayes_model.fit(one_hot_features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Mja_sSrJ41lh"
   },
   "outputs": [],
   "source": [
    "# Evaluate on the testing data \n",
    "preds = best_bayes_model.predict_proba(one_hot_features_test)[:, 1]\n",
    "print('The best model from Bayes optimization scores {:.5f} AUC ROC on the test set.'.format(roc_auc_score(test_labels, preds)))\n",
    "print('This was achieved after {} search iterations'.format(results.loc[0, 'iteration']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NILzHVYu41li"
   },
   "source": [
    "If we want to save the results, we can use the json file format. Saving the trials results will allow us to continue the hyperparameter search where we left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "FyDm-ivb41lj"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save the trial results\n",
    "with open('trials_kaggle.json', 'w') as f:\n",
    "    f.write(json.dumps(trials_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZWpHi42941lo"
   },
   "source": [
    "## Visualizing Hyperparameters\n",
    "\n",
    "One interesting thing we can do with the results is to see the different hyperparameters tried by both random search and the Tree Parzen Estimator. Since random search is choosing without regards to the previous results, we would expect that the distribution of samples should be close to the domain space we defined (it won't be exact since we are using a fairly small number of iterations). On the other hand, the Bayes Optimization, if given enough time, should concetrate on the \"more promising\" hyperparameters. \n",
    "\n",
    "In addition to a more concentrated search, we expect that the average validation loss of the Bayesian Optimization should be lower than that on the random search because it chooses values likely (according to the probability model) to yield lower losses on the objective function. The validation loss should also decrease over time with the Bayesian method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h-itZe2l41lo"
   },
   "source": [
    "First we will need to extract the hyperparameters from both search methods. We will store these in separate dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "QGyF2SNC41lo"
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe for storing parameters\n",
    "random_params = pd.DataFrame(columns = list(random_results.loc[0, 'params'].keys()),\n",
    "                            index = list(range(len(random_results))))\n",
    "\n",
    "# Add the results with each parameter a different column\n",
    "for i, params in enumerate(random_results['params']):\n",
    "    random_params.loc[i, :] = list(params.values())\n",
    "    \n",
    "random_params['loss'] = random_results['loss']\n",
    "random_params['iteration'] = random_results['iteration']\n",
    "random_params.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "lKv1cptJ41lr"
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe for storing parameters\n",
    "bayes_params = pd.DataFrame(columns = list(ast.literal_eval(results.loc[0, 'params']).keys()),\n",
    "                            index = list(range(len(results))))\n",
    "\n",
    "# Add the results with each parameter a different column\n",
    "for i, params in enumerate(results['params']):\n",
    "    bayes_params.loc[i, :] = list(ast.literal_eval(params).values())\n",
    "    \n",
    "bayes_params['loss'] = results['loss']\n",
    "bayes_params['iteration'] = results['iteration']\n",
    "\n",
    "bayes_params.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ttEPsJcz41lt"
   },
   "source": [
    "#### Learning Rates\n",
    "\n",
    "The first plot shows the sampling distribution, random search, and Bayesian optimization learning rate distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "-rChGyZ-41lt"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 8))\n",
    "plt.rcParams['font.size'] = 18\n",
    "\n",
    "# Density plots of the learning rate distributions \n",
    "sns.kdeplot(learning_rate_dist, label = 'Sampling Distribution', linewidth = 4)\n",
    "sns.kdeplot(random_params['learning_rate'], label = 'Random Search', linewidth = 4)\n",
    "sns.kdeplot(bayes_params['learning_rate'], label = 'Bayes Optimization', linewidth = 4)\n",
    "plt.vlines([best_random_params['learning_rate'], best_bayes_params['learning_rate']],\n",
    "           ymin = 0.0, ymax = 50.0, linestyles = '--', linewidth = 4, colors = ['orange', 'green'])\n",
    "plt.legend()\n",
    "plt.xlabel('Learning Rate'); plt.ylabel('Density'); plt.title('Learning Rate Distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5vY4yN4c41lu"
   },
   "source": [
    "#### Boosting Type \n",
    "\n",
    "Random search should use the boosting types with the same frequency. However, Bayesian Optimization might have decided (modeled) that one boosting type is better than another for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "mT9CruzS41lv"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharey = True, sharex = True)\n",
    "\n",
    "# Bar plots of boosting type\n",
    "random_params['boosting_type'].value_counts().plot.bar(ax = axs[0], figsize = (14, 6), color = 'orange', title = 'Random Search Boosting Type')\n",
    "bayes_params['boosting_type'].value_counts().plot.bar(ax = axs[1], figsize = (14, 6), color = 'green', title = 'Bayes Optimization Boosting Type');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "dig7Mq4f41ly"
   },
   "outputs": [],
   "source": [
    "print('Random Search boosting type percentages')\n",
    "100 * random_params['boosting_type'].value_counts() / len(random_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ReKRrsmW41lz"
   },
   "outputs": [],
   "source": [
    "print('Bayes Optimization boosting type percentages')\n",
    "100 * bayes_params['boosting_type'].value_counts() / len(bayes_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Jv9wiLP41l1"
   },
   "source": [
    "### Plots of All Numeric Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Px7WiG_R41l1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Iterate through each hyperparameter\n",
    "for i, hyper in enumerate(random_params.columns):\n",
    "    if hyper not in ['class_weight', 'boosting_type', 'iteration', 'subsample', 'metric', 'verbose', 'loss', 'learning_rate', 'encoding']:\n",
    "        plt.figure(figsize = (14, 6))\n",
    "        # Plot the random search distribution and the bayes search distribution\n",
    "        if hyper != 'loss':\n",
    "            sns.kdeplot([sample(space[hyper]) for _ in range(1000)], label = 'Sampling Distribution', linewidth = 4)\n",
    "        sns.kdeplot(random_params[hyper], label = 'Random Search', linewidth = 4)\n",
    "        sns.kdeplot(bayes_params[hyper], label = 'Bayes Optimization', linewidth = 4)\n",
    "        plt.vlines([best_random_params[hyper], best_bayes_params[hyper]],\n",
    "                     ymin = 0.0, ymax = 10.0, linestyles = '--', linewidth = 4, colors = ['orange', 'green'])\n",
    "        plt.legend(loc = 1)\n",
    "        plt.title('{} Distribution'.format(hyper))\n",
    "        plt.xlabel('{}'.format(hyper)); plt.ylabel('Density');\n",
    "        plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YBQtv58R41l3"
   },
   "source": [
    "## Evolution of Hyperparameters\n",
    "\n",
    "We can also plot the hyperparameters over time (against the number of iterations) to see how they change for the Bayes Optimization. First we will map the `boosting_type` to an integer for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "jiybnOSy41l4"
   },
   "outputs": [],
   "source": [
    "# Map boosting type to integer (essentially label encoding)\n",
    "bayes_params['boosting_int'] = bayes_params['boosting_type'].replace({'gbdt': 1, 'goss': 2, 'dart': 3})\n",
    "\n",
    "# Plot the boosting type over the search\n",
    "plt.plot(bayes_params['iteration'], bayes_params['boosting_int'], 'ro')\n",
    "plt.yticks([1, 2, 3], ['gdbt', 'goss', 'dart']);\n",
    "plt.xlabel('Iteration'); plt.title('Boosting Type over Search');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "_AqdaAgL41l5"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize = (24, 6))\n",
    "i = 0\n",
    "\n",
    "# Plot of four hyperparameters\n",
    "for i, hyper in enumerate(['colsample_bytree', 'learning_rate', 'min_child_samples', 'num_leaves']):\n",
    "    \n",
    "        # Scatterplot\n",
    "        sns.regplot('iteration', hyper, data = bayes_params, ax = axs[i])\n",
    "        axs[i].scatter(best_bayes_params['iteration'], best_bayes_params[hyper], marker = '*', s = 200, c = 'k')\n",
    "        axs[i].set(xlabel = 'Iteration', ylabel = '{}'.format(hyper), title = '{} over Search'.format(hyper));\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Fv09_iZi41l7"
   },
   "outputs": [],
   "source": [
    "best_bayes_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Z80y7MBq41l9"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize = (18, 6))\n",
    "i = 0\n",
    "\n",
    "# Scatterplot of next three hyperparameters\n",
    "for i, hyper in enumerate(['reg_alpha', 'reg_lambda', 'subsample_for_bin']):\n",
    "        sns.regplot('iteration', hyper, data = bayes_params, ax = axs[i])\n",
    "        axs[i].scatter(best_bayes_params['iteration'], best_bayes_params[hyper], marker = '*', s = 200, c = 'k')\n",
    "        axs[i].set(xlabel = 'Iteration', ylabel = '{}'.format(hyper), title = '{} over Search'.format(hyper));\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oca_dh2041mD"
   },
   "source": [
    "#### Validation Losses\n",
    "\n",
    "Finally, we can look at the losses recorded by both random search and Bayes Optimization. We would expect the average loss recorded by Bayes Optimization to be lower because this method should spend more time in promising regions of the search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "qTzVDqin41mE"
   },
   "outputs": [],
   "source": [
    "# Dataframe of just scores\n",
    "scores = pd.DataFrame({'ROC AUC': 1 - random_params['loss'], 'iteration': random_params['iteration'], 'search': 'random'})\n",
    "scores = scores.append(pd.DataFrame({'ROC AUC': 1 - bayes_params['loss'], 'iteration': bayes_params['iteration'], 'search': 'Bayes'}))\n",
    "\n",
    "scores['ROC AUC'] = scores['ROC AUC'].astype(np.float32)\n",
    "scores['iteration'] = scores['iteration'].astype(np.int32)\n",
    "\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "brGHZkGO41mH"
   },
   "source": [
    "We can make histograms of the scores (not taking in account the iteration) on the same x-axis scale to see if there is a difference in scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "L0iTQzCP41mH"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18, 6))\n",
    "\n",
    "# Random search scores\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(1 - random_results['loss'].astype(np.float64), label = 'Random Search', edgecolor = 'k');\n",
    "plt.xlabel(\"Validation ROC AUC\"); plt.ylabel(\"Count\"); plt.title(\"Random Search Validation Scores\")\n",
    "plt.xlim(0.75, 0.78)\n",
    "\n",
    "# Bayes optimization scores\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(1 - bayes_params['loss'], label = 'Bayes Optimization', edgecolor = 'k');\n",
    "plt.xlabel(\"Validation ROC AUC\"); plt.ylabel(\"Count\"); plt.title(\"Bayes Optimization Validation Scores\");\n",
    "plt.xlim(0.75, 0.78);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gLOtroJH41mK"
   },
   "source": [
    "It does appear that the validation ROC AUC for the Bayesian optimization is higher than that for Random Search. However, as we have seen, this does not necessarily translate to a better testing score! \n",
    "\n",
    "Bayesian optimization should get better over time. Let's plot the scores against the iteration to see if there was improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rKbGgrtE41mK"
   },
   "outputs": [],
   "source": [
    "# Plot of scores over the course of searching\n",
    "sns.lmplot('iteration', 'ROC AUC', hue = 'search', data = scores, size = 8);\n",
    "plt.scatter(best_bayes_params['iteration'], 1 - best_bayes_params['loss'], marker = '*', s = 400, c = 'orange', edgecolor = 'k')\n",
    "plt.scatter(best_random_params['iteration'], 1 - best_random_params['loss'], marker = '*', s = 400, c = 'blue', edgecolor = 'k')\n",
    "plt.xlabel('Iteration'); plt.ylabel('ROC AUC'); plt.title(\"Validation ROC AUC versus Iteration\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eFc4-Zkp41mQ"
   },
   "source": [
    "It's reassuring to see that the validation ROC AUC scores of Bayesian optimization increase over time. What this shows is that the model is exploring hyperparameters that are better according to the cross validation metric! It would be interesting to continue searching and see if there is a plateau in the validation scores (there would have to be eventually). Moreover, even if validation scores continue to increase, that does not mean a better model for the testing data! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "EbdLlxye41mS"
   },
   "outputs": [],
   "source": [
    "# Save dataframes of parameters\n",
    "bayes_params.to_csv('results/bayes_params_kaggle.csv', index = False)\n",
    "random_params.to_csv('results/random_params_kaggle.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DJRav36m41mU"
   },
   "source": [
    "## Continue Searching\n",
    "\n",
    "We can keep running the Bayesian hyperparameter search for more iterations to try for better results. Hyperopt will continue searching where it left off if we [pass it a trials object that already has information on previous runs](https://github.com/hyperopt/hyperopt/issues/267). This raises a good point: always save your previous results, because you never know when they will be useful! \n",
    "\n",
    "Another interesting point to not is that Bayesian Optimization methods do not have any internal state which means all they need are the results: previous inputs to the objective function and the resulting loss. Based only on these results, these methods can construct a surrogate function and suggest the next set of hyperparameters to evaluate. The internals of the objective function have no effect on the Bayesian Optimization method hence the naming of this as a black box optimization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "dR6ri9jz41mW"
   },
   "outputs": [],
   "source": [
    "# Continue training\n",
    "ITERATION = MAX_EVALS + 1\n",
    "\n",
    "# Set more evaluations\n",
    "MAX_EVALS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "AFLJzTYP41mZ"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Use the same trials object to keep training\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, \n",
    "            max_evals = MAX_EVALS, trials = bayes_trials, verbose = 1, \n",
    "            rstate = np.random.RandomState(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Y34KhpAz41ma"
   },
   "outputs": [],
   "source": [
    "# Sort the trials with lowest loss (highest AUC) first\n",
    "bayes_trials_results = sorted(bayes_trials.results, key = lambda x: x['loss'])\n",
    "bayes_trials_results[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "bz0E53Tl41mc"
   },
   "outputs": [],
   "source": [
    "results = pd.read_csv('results/gbm_trials_kaggle.csv')\n",
    "\n",
    "# Sort values with best on top and reset index for slicing\n",
    "results.sort_values('loss', ascending = True, inplace = True)\n",
    "results.reset_index(inplace = True, drop = True)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "UdV7Da4U41mf"
   },
   "outputs": [],
   "source": [
    "# Extract the ideal number of estimators and hyperparameters\n",
    "best_bayes_estimators = int(results.loc[0, 'estimators'])\n",
    "best_bayes_params = ast.literal_eval(results.loc[0, 'params']).copy()\n",
    "\n",
    "# Re-create the best model and train on the training data\n",
    "best_bayes_model = lgb.LGBMClassifier(n_estimators=best_bayes_estimators, n_jobs = -1, \n",
    "                                       objective = 'binary', random_state = 50, **best_bayes_params)\n",
    "best_bayes_model.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5vfo-OoC41mh"
   },
   "outputs": [],
   "source": [
    "# Evaluate on the testing data \n",
    "preds = best_bayes_model.predict_proba(test_features)[:, 1]\n",
    "print('The best model from Bayes optimization scores {:.5f} AUC ROC on the test set.'.format(roc_auc_score(test_labels, preds)))\n",
    "print('This was achieved after {} search iterations'.format(results.loc[0, 'iteration']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qZ3RXr3Z41mi"
   },
   "source": [
    "The continuation of the search did slightly improve the validation score (again depending on training run). Instead of training more, we might want to restart the search so the algorithm can spend more time exploring the domain space. As searching continues, the algorithm shifts from exploring (trying new values) to exploiting (trying those values that worked best in the past). This is generally what we want unless the model gets stuck in a local minimum at which point we would want to restart the search in a different region of the hyperparameter space. Bayesian Optimization of hyperparameters is still prone to overfitting, even when using cross-validation because it can get settle into a local minimum of the objective function. It is very difficult to tell when this occurs for a high-dimensional problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Abkdy7w41mi"
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook, we saw how to implement automated hyperparameter tuning with Bayesian Optimization methods. We used the open-source Python library Hyperopt with the Tree Parzen Estimator to optimize the hyperparameters of a gradient boosting machine. \n",
    "\n",
    "Bayesian model-based optimization can be more efficient than random search, finding a better set of model hyperparameters in fewer search iterations (although not in every case). However, just because the model hyperparameters are better on the validation set does not mean they are better for the testing set! For this training run, Bayesian Optimization found a better set of hyperparamters according to the validation and the test data although the testing score was much lower than the validation ROC AUC. This is a useful lesson that even when using cross-validation, overfitting is still one of the top problems in machine learning. \n",
    "\n",
    "Bayesian optimization  is a powerful technique that we can use to tune any machine learning model, so long as we can define an objective function that returns a value to minimize and a domain space over which to search. This can extend to any function that we want to minimize (not just hyperparameter tuning). Bayesian optimization can be a significant upgrade over uninformed methods such as random search and because of the ease of use in Python are now a good option to use for hyperparameter tuning. As with most subjects in machine learning, there is no single best answer for hyperparameter tuning, but Bayesian optimization methods should be a tool that helps data scientists with the tedious but necessary task of model tuning! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "N-jKolSM41mi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "8MTm-NB741io",
    "QRRr4Y2s41jN",
    "k3oxk-2H41jZ",
    "_V-1xJEu41jj",
    "bifzcnYA41jn",
    "UskS2wb041jt",
    "5rAB7_7N41j_",
    "e0sRGTWN41kM",
    "ODmo-6_o41kS",
    "Uk4yYULV41kr",
    "FULCSPXR41le",
    "ZWpHi42941lo",
    "ttEPsJcz41lt",
    "5vY4yN4c41lu",
    "6Jv9wiLP41l1",
    "YBQtv58R41l3",
    "Oca_dh2041mD"
   ],
   "name": "Kaggle Version of Bayesian Hyperparameter Optimization of GBM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python (py35clone)",
   "language": "python",
   "name": "py35clone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
